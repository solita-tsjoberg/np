---
title: "Neurala Nätverk i R med TensorFlow"
author: "Torbjörn Sjöberg"
date: "1/19/2022"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, fig.align='center')
```

I den här övningen kommer vi att använda [TensorFlow](https://www.tensorflow.org/), ett av de mest populära paketen för att bygga neurala nätverk.
Övningen är tönkt för att visa up snabbt hur enkelt det är att bygga sina egna neurala nätverk. För mer exempel så rekommenderas [RStudios TensorFlow tutorials](https://tensorflow.rstudio.com/tutorials/).

## Installation
TensorFlow är ett Open Source paket som är skapat av Google och är skrivet i en kombination av Python, C++ och CUDA.
Det är alltså inte som andra R paket som är skrivna i R och kräver lite extra installation.

Notera att i den här övningen så har en Mac (Unix) använts till installation, för att installera på windows krävs en installation av
[Anaconda](https://www.anaconda.com/). På [Den här sidan](https://tensorflow.rstudio.com/installation/) finns en komplett installationsguide TensorFlow i R.


```{r install_tf, eval=FALSE}
# Installera R paket
install.packages('tensorflow')

# läsa in paketet
library(tensorflow)

# Installera det R-oberoende TensorFlow programmet på din dator
install_tensorflow()

```

```{r load, echo=FALSE, eval=TRUE}
# för att se till att tensorflow laddas när vi knittar markdown filen.
library(tensorflow)
```

```{r test_tensorflow}
# testa att installationen fungerat korrekt
tf$constant(1)
```

Om installationen har fungerat korrekt så ska outputen vara `tf.Tensor(1.0, shape=(), dtype=float32)´.

## Data Prep

Innan vi bygger vårt neurala nåtverk behöver vi data att jobba med! Keras, en frontend till TensorFlow, kommer packeterat med några exempel-dataset som vi kan använda.
Ett av dem ör MNIST datasetet som är en samling handskrivna bokstäver och är ett av de mest klassiska dataseten för att testa på ML.

```{r load_packages, include=FALSE}
#Vi behöver ladda våra paket innan vi använder dem

library(tidyverse) #Laddar in många av de vanliga R paketen
library(keras) #Frontend till TensorFlow.
library(reshape2) #Används för att enklare visa upp vår data
```

Vi laddar in och skapar sedan4 stycken arrayer (listor):
- `train_images` - Vår träningsdata (X), 60 000 observationer med 28x28 svartvita bilder med siffrorna
- `train_labels`, Våra träningsetiketter (Y) som berättar vilken siffra träningsdatat ska representera
- `test_images`, 10 000 träningsbilder för att testa modellen
- `test_labels`, 10 000 etiketter för att testa modelle

```{r load_data}
mnist <- dataset_mnist()

# Dataseten från keras kommer i vektorer som indelade i test och train-set.
# Vi skapar inte ett valideringsset här utan det görs automatiskt när vi tränar modellen senare.
train_images <- mnist$train$x / 255
train_labels <- mnist$train$y
test_images <- mnist$test$x / 255
test_labels <- mnist$test$y
```

```{r helper_functions, include=FALSE}
# Hjälpfunktion för att rotera matriser 90 grader
# Datan kommer i ett 90 grader roterat format, vilket inte spelar någor roll för NN
# Men för våra visualieringar
rotate <- function(x) t(apply(x, 2, rev))

# Det här är en hjälpfunktion för att visa upp hur mnist datasetet ser ut
plot_mnist <- function(mnist_matrix) {
  mnist_matrix_rot <- mnist_matrix %>%
    rotate()
  image(1:28, 1:28, mnist_matrix_rot, col = gray((0:255)/255))
}
```
För att se hur datan ser ut så kan man köra code chunken här under:

```{r display_random_mnist}
number_to_display <- sample(1:dim(train_images)[1], 1) #Sampla siffra mellan 1 och antalet träningsexempel
cat('Bild nummer', number_to_display, 'som föreställer siffran', train_labels[number_to_display], ':\n\n')
```
```{r echo=FALSE}
plot_mnist(train_images[number_to_display, ,])
```

Nu har vi nästan kommit hela vägen fram till att börja bygga våra neurala nätverk, men vi behöver först forma om datan lite till för att
passa formaten som de neurala nätverken kräver.

Det gör vi genom att göra om våra labels till en one hot encoding av våra labels, och platta ut vatriser till rader

```{r reshape_data}
# One hot encoding
num_classes = 10
train_labels_cat <- train_labels %>% to_categorical(num_classes)
test_labels_cat <- test_labels %>% to_categorical(num_classes)

# Flatten matrices
train_images_flat <- array_reshape(train_images, c(nrow(train_images), 784))
test_images_flat <- array_reshape(test_images, c(nrow(test_images), 784))

# För säkerhets skull kan vi slumpa ordningen på vårt dataset
set.seed(123)
new_order <- sample(nrow(train_images_flat))
train_images_flat <- train_images_flat[new_order, ]
train_labels_cat <- train_labels_cat[new_order, ]

```


### Bygga DNN

Nu är vi redo att bygga vårt neurala nätverk!
Vi använder här en `sequential` keras modell, vilket betyder att vi definierar modellen ett lager i taget.

```{r}

model_nn <-
  # Initiera modellen
  keras_model_sequential() %>%

    # Vårt första hidden layer, kräver att vi har rätt input dimensioner
    layer_dense(
      units = 16, #Antal noder
      input_shape = ncol(train_images_flat),
      activation = 'relu' #Aktiveringsfunktion
    ) %>%

    # Ett till lager
    layer_dense(
      units = 16, #Antal noder
      activation = 'relu' #Aktiveringsfunktion
    ) %>%

    # Output lager
    layer_dense(
      units = 10, # lika många som vi har kategorier
      activation = 'softmax' #Motsvarar Logit, för flera klasser
    )
```

För att se hur modellen ser ut kan vi använda `summary()`.
Här kan vi se att vi i det här relativt enkla nätverket med 2 gömda lager med 16 noder var, har 13 000 parametrar att jobba med!

Antalet parametrar växer fort i NN!

```{r}
model_nn %>% summary()
```

Sedan behöver vi "kompilera" modellen och där specificera vår loss funktion, optimeringsalgorims, och mätetal.

```{r}
model_nn %>%
  compile(
    loss = 'categorical_crossentropy', #Motsvarar binary cross entropy för multi-class
    optimizer = optimizer_adam(), # Här använder vi en lite mer avancerad optimerare än
                                  # bara SGD eftersom träningen går mycket snabbare.
    metrics = c('accuracy', metric_precision(), metric_recall()) #
  )

```

Sedan kan vi starta träningen av modellen. Notera här parametern `validation_split`,
den skapar det valideringsset som används vi kan använda för att jämföra olika
architekturer av NNs eller andra modeller.

I den här implementationen så väljs valideringssetet från slutet av träningsetet.

### Träna nätverket

Nu är det bara att köra vår modell med fit funktionen, under tiden den kör så kan vi se hur
vår loss och våra mätetal/metrics utvecklas under träningen.
```{r}
history <- model_nn %>%
  fit(
    x = train_images_flat,
    y = train_labels_cat,
    epochs = 30, # Varje datapunkt tränas på 30 ggr
    batch_size = 128, # 128 datapunkter betygsätts tillsammans mellan uppdateringar
    validation_split = 0.2,
    callbacks = callback_tensorboard('logs/run_a')
  )
```

```{r echo=FALSE}
history %>% plot()
history
```

### Slutgiltig evaluering och stickprov
Nu är modellen tränad och om vi är nöjda med vår performance (Med en validation accuracy på 95-96 % är det ändå ganska bra!) så kan vi titta på
om det verkar ha generaliserats till vårt test-dataset.

```{r}
score <- model_nn %>% evaluate(test_images_flat, test_labels_cat, callbacks = callback_tensorboard('logs/run_a'))
score

# skapa predictioner på test data setet
predictions <- model_nn %>% predict(test_images_flat)
colnames(predictions) <- 0:9

# Göra siffror från one hot enc
predictions_num <- predictions %>%
  as_tibble() %>%
  mutate(
    pred_num=max.col(predictions)-1, 
    max_prediction = do.call(pmax, (.))
  ) %>%
  select(pred_num, max_prediction)

# Kombinera Predictions med våra test labels
test_labels_real_pred <- tibble(image_index = 1:nrow(test_labels), test_labels, predictions_num)
```

Fortfarande kring ca 95 %!
Hur ser det ut om vi tar stickprov i vårt testset?

```{r}
rand_num <- sample(1:nrow(test_labels_real_pred), size = 1)
cat('Testbild nummer', rand_num, 'som föreställer siffran', test_labels_real_pred$test_labels[rand_num], ':\n')
cat(
  'får en predicerad siffra:',
  test_labels_real_pred$pred_num[rand_num],
  'med säkerhet:',
  test_labels_real_pred$max_prediction[rand_num]
)
 
plot_mnist(test_images[rand_num, ,])
```

Och i tabellformat:
```{r}
# 10 slumpmässigt valda testblider, deras sanna och predicerade labels, och den predicerade säkerheten det neurala nätverket hade.
test_labels_real_pred %>% slice_sample(n = 10)
```

